{"$defs": {"Author": {"properties": {"first": {"description": "first name and any middle name or initials", "title": "First", "type": "string"}, "last": {"description": "last name", "title": "Last", "type": "string"}, "required": ["first", "last"], "title": "Author", "type": "object"}, "required": ["first", "last"], "title": "Author", "type": "object"}, "keywords": {"description": "List of keywords.", "items": {"type": "string"}, "title": "Keywords", "type": "array"}, "abstract": {"anyOf": [{"type": "string"}, {"type": "null"}], "description": "the abstract of the article", "title": "Abstract"}}, "required": ["title", "authors", "keywords", "abstract"], "title": "Shape Matters: Understanding the Implicit Bias of the Noise Covariance", "authors": [{"first": "Jeff Z.", "last": "Hao Chen"}, {"first": "Colin", "last": "Wei"}, {"first": "Jason D.", "last": "Lee"}, {"first": "Tengyu", "last": "Ma"}], "keywords": ["implicit bias", "noise covariance", "stochastic gradient descent", "overparameterized models", "label noise", "parameter-dependent noise", "implicit regularization", "deep neural networks"], "abstract": "The noise in stochastic gradient descent (SGD) provides a crucial implicit regularization effect for training overparameterized models. Prior theoretical work largely focuses on spherical Gaussian noise, whereas empirical studies demonstrate the phenomenon that parameter-dependent noise \u2014 induced by mini-batches or label perturbation \u2014 is far more effective than Gaussian noise. This paper theoretically characterizes this phenomenon on a quadratically-parameterized model introduced by Vaskevicius et al. (2019) and Woodworth et al. (2020). We show that in an over-parameterized setting, SGD with label noise recovers the sparse ground-truth with an arbitrary initialization, whereas SGD with Gaussian noise or gradient descent overfits to dense solutions with large norms. Our analysis reveals that parameter-dependent noise introduces a bias towards local minima with smaller noise variance, whereas spherical Gaussian noise does not. Code for our project is publicly available.1", "output": {"title": "Shape Matters: Understanding the Implicit Bias of the Noise Covariance", "authors": [{"first": "Jeff Z.", "last": "Hao Chen"}, {"first": "Colin", "last": "Wei"}, {"first": "Jason D.", "last": "Lee"}, {"first": "Tengyu", "last": "Ma"}], "abstract": "The noise in stochastic gradient descent (SGD) provides a crucial implicit regularization effect for training overparameterized models. Prior theoretical work largely focuses on spherical Gaussian noise, whereas empirical studies demonstrate the phenomenon that parameter-dependent noise \u2014 induced by mini-batches or label perturbation \u2014 is far more effective than Gaussian noise. This paper theoretically characterizes this phenomenon on a quadratically-parameterized model introduced by Vaskevicius et al. (2019) and Woodworth et al. (2020). We show that in an over-parameterized setting, SGD with label noise recovers the sparse ground-truth with an arbitrary initialization, whereas SGD with Gaussian noise or gradient descent overfits to dense solutions with large norms. Our analysis reveals that parameter-dependent noise introduces a bias towards local minima with smaller noise variance, whereas spherical Gaussian noise does not. Code for our project is publicly available.1", "keywords": ["implicit bias", "noise covariance", "stochastic gradient descent", "overparameterized models", "label noise", "parameter-dependent noise", "implicit regularization", "deep neural networks"], "output": "https://github.com/jhaochenz/noise-implicit-bias"}}