{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781ed4eb-c0ed-4448-b598-0e54e8f7ffa8",
   "metadata": {},
   "source": [
    "# Stanford Preprints\n",
    "\n",
    "This notebook downloads Stanford preprints from [OpenAlex](https://openalex.org).\n",
    "\n",
    "If we query OpenAlex directly using [has_oa_submitted_version](https://docs.openalex.org/api-entities/works/filter-works#has_oa_submitted_version) we can get all preprints, irregardless of whether they have a DOI assigned to them. But that also means we will need to use the openalex ID to save the PDF instead of the DOI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f5c167-b109-457a-add0-99567411d980",
   "metadata": {},
   "source": [
    "## Institution Codes\n",
    "\n",
    "First we need to know what institution codes to use for Stanford. Those can be collected from the API, and we can see the number of pre-print publications for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "badc3aeb-6cc8-4e1a-a4f9-3dbea1c5e5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford University (I97018004): 62331\n",
      "Stanford Medicine (I4210137306): 1936\n",
      "Stanford Synchrotron Radiation Lightsource (I4210120900): 7052\n",
      "Stanford Health Care (I4210105015): 254\n",
      "SRI International (I1298353152): 1359\n",
      "Stanford Blood Center (I4210133340): 46\n",
      "SLAC National Accelerator Laboratory (I2801935854): 15206\n",
      "Stanford Cancer Institute (I4390039303): 0\n",
      "Stanford SystemX Alliance (I4392738099): 0\n",
      "Stanford Maternal and Child Health Research Institute (I4391767688): 0\n",
      "Institute for Stem Cell Biology and Regenerative Medicine (I4394709089): 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pyalex\n",
    "\n",
    "inst_codes = []\n",
    "\n",
    "for inst in pyalex.Institutions().search(\"stanford\").get():\n",
    "    inst_id = re.sub(r'https://.+/', '', inst['id'])\n",
    "    count = pyalex.Works().filter(institutions={\"id\": inst_id}, has_oa_submitted_version=True).count()\n",
    "    print(f\"{inst['display_name']} ({inst_id}): {count}\")\n",
    "    inst_codes.append(inst_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19cb93-2099-4b03-85d7-703d0a14ed1b",
   "metadata": {},
   "source": [
    "We can query them all with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "249ba224-e9e3-407f-8230-5d6ab61ebd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70133"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyalex.Works().filter(institutions={\"id\": '|'.join(inst_codes)}, has_oa_submitted_version=True).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e38ec-77c0-4ad4-af00-9fdee8d64e13",
   "metadata": {},
   "source": [
    "## Download PDFs\n",
    "\n",
    "So we want to download the PDFs for these works. Here's a (kinda complex) download function that takes an OpenAlex Work (dictionary) and downloads the PDF to the filesystem using the domain of the PDF URL and the OpenAlex ID as a file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ecab74-64ac-4047-bcc2-8492c83cfa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('preprints/arxiv.org/W2117539524.pdf')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "# we are fetching with SSL verification off because requests doesn't seem to like www.slac.stanford.edu (at least for me)\n",
    "requests.packages.urllib3.disable_warnings() \n",
    "\n",
    "# use a User-Agent that looks legit, but has my email address at the end.\n",
    "# Experimentation showed that some servers respond differently based on the User Agent.\n",
    "\n",
    "ua = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:130.0) Gecko/20100101 Firefox/130.0\"\n",
    "\n",
    "download_dir = pathlib.Path('preprints')\n",
    "\n",
    "def download_work(work, sleep_seconds=0.5):\n",
    "    preprints = [l for l in work['locations'] if l['version'] == 'submittedVersion']\n",
    "\n",
    "    if len(preprints) == 0:\n",
    "        print(f\"No preprint location found for {work['id']}\")\n",
    "\n",
    "    pdf_url = preprints[0]['pdf_url']\n",
    "    if pdf_url is None:\n",
    "        print(f\"No preprint pdf_url found for {work['id']}\")\n",
    "        return None\n",
    "\n",
    "    # construct the pdf filename\n",
    "    domain = urlparse(pdf_url).netloc\n",
    "    filename = download_dir / domain / (work['id'].split('/')[-1] + \".pdf\")\n",
    "\n",
    "    # sleep to not overwhelm servers by accident\n",
    "    time.sleep(sleep_seconds)\n",
    "\n",
    "    # get the pdf, only waiting 30 seconds for the response\n",
    "    try:\n",
    "        resp = requests.get(pdf_url, allow_redirects=True, timeout=30, verify=False, headers={\"User-Agent\": ua})\n",
    "    except Exception as e:\n",
    "        print(f\"exception when fetching {pdf_url} - {e}\")\n",
    "        return None\n",
    "\n",
    "    # If it's a 200 OK response and looks like a PDF write it to the filesystem using the DOI.\n",
    "    # Note: some servers serve up PDFs using binary/octet-stream \n",
    "    # e.g. https://figshare.com/articles/journal_contribution/Estimation_and_Inference_of_Heterogeneous_Treatment_Effects_using_Random_Forests_sup_sup_/4902002/2/files/8238752.pdf \n",
    "    \n",
    "    content_type = resp.headers.get('Content-Type', '').lower()\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"got {resp.status_code} from {pdf_url}\")\n",
    "        return None\n",
    "    elif 'pdf' in content_type or 'octet-stream' in content_type:\n",
    "        filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "        filename.open('wb').write(resp.content)\n",
    "        return filename\n",
    "    else:\n",
    "        print(f\"{pdf_url} didn't look like a application/pdf response\")\n",
    "        return None\n",
    "\n",
    "download_work(pyalex.Works().filter(institutions={\"id\": \"I97018004\"}, has_oa_submitted_version=True).get()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e30bb24-a30d-44fb-9a56-9d23ad8c345a",
   "metadata": {},
   "source": [
    "Now we can download all the PDFs again while writing the metadata file. Note, this took 3 days to complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7660dea-71bc-4ed7-883b-088bb528384f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "\n",
    "metadata_file = download_dir / \"metadata.jsonl\"\n",
    "jsonl = metadata_file.open('a')\n",
    "\n",
    "works = pyalex.Works().filter(institutions={\"id\": \"I97018004\"}, has_oa_submitted_version=True)\n",
    "for page in tqdm.tqdm(works.paginate(per_page=100, n_max=None), total=(works.count() / 100)):\n",
    "    for work in page:            \n",
    "        pdf_file = download_work(work)\n",
    "        \n",
    "        # only write the metadata if we were able to download a PDF\n",
    "        if pdf_file:\n",
    "            work['download_file'] = re.sub('^preprints/', '', str(pdf_file))\n",
    "            jsonl.write(json.dumps(work) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
