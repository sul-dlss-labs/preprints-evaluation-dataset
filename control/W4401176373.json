{
  "title": "CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving",
  "authors": [
    {
      "first": "Yuhan",
      "last": "Liu"
    },
    {
      "first": "Hanchen",
      "last": "Li"
    },
    {
      "first": "Yihua",
      "last": "Cheng"
    },
    {
      "first": "Siddhant",
      "last": "Ray"
    },
    {
      "first": "Yuyang",
      "last": "Huang"
    },
    {
      "first": "Qizheng",
      "last": "Zhang"
    },
    {
      "first": "Kuntai",
      "last": "Du"
    },
    {
      "first": "Jiayi",
      "last": "Yao"
    },
    {
      "first": "Shan",
      "last": "Lu"
    },
    {
      "first": "Ganesh",
      "last": "Ananthanarayanan"
    },
    {
      "first": "Michael",
      "last": "Maire"
    },
    {
      "first": "Henry",
      "last": "Hoffmann"
    },
    {
      "first": "Ari",
      "last": "Holtzman"
    },
    {
      "first": "Junchen",
      "last": "Jiang"
    }
  ],
  "abstract": "As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge. Yet using long contexts is challenging as nothing can be generated until the whole context is processed by the LLM. While the context-processing delay can be reduced by reusing the KV cache of a context across different inputs, fetching the KV cache, which contains large tensors, over the network can cause high extra network delays. CacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, leveraging KV cache's distributional properties to encode a KV cache into more compact bitstream representations with negligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts the compression level of different parts of a KV cache to cope with changes in available bandwidth, in order to maintain low context-loading delay and high generation quality. We test CacheGen on popular LLMs and datasets. Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x and the total delay in fetching and processing contexts by 3.2-3.7x with negligible impact on the LLM response quality. Our code is at: https://github.com/UChi-JCL/CacheGen.",
  "keywords": [
    "Large Language Models",
    "KV Cache",
    "Compression"
  ]
}
